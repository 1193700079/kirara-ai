import asyncio

import aiohttp
import requests
from pydantic import BaseModel, ConfigDict

from kirara_ai.llm.adapter import AutoDetectModelsProtocol, LLMBackendAdapter
from kirara_ai.llm.format.message import LLMChatImageContent, LLMChatMessage, LLMChatTextContent
from kirara_ai.llm.format.request import LLMChatRequest
from kirara_ai.llm.format.response import LLMChatResponse, Message, Usage
from kirara_ai.media import MediaManager


async def convert_llm_chat_message_to_openai_message(msg: LLMChatMessage, media_manager: MediaManager) -> dict:
    parts = []
    for element in msg.content:
        if isinstance(element, LLMChatTextContent):
            parts.append(element.model_dump(mode="json"))
        elif isinstance(element, LLMChatImageContent):
            media = media_manager.get_media(element.media_id)
            parts.append(
                {
                    "type": "image_url",
                    "image_url": {
                        "url": await media.get_url()
                    }
                }
            )
    return parts

class OpenAIConfig(BaseModel):
    api_key: str
    api_base: str = "https://api.openai.com/v1"
    model_config = ConfigDict(frozen=True)


class OpenAIAdapter(LLMBackendAdapter, AutoDetectModelsProtocol):
    media_manager: MediaManager
    
    def __init__(self, config: OpenAIConfig):
        self.config = config

    def chat(self, req: LLMChatRequest) -> LLMChatResponse:
        api_url = f"{self.config.api_base}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json",
        }
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        contents = loop.run_until_complete(
            asyncio.gather(
                *[convert_llm_chat_message_to_openai_message(msg, self.media_manager) for msg in req.messages]
            )
        )

        data = {
            "messages": contents,
            "model": req.model,
            "frequency_penalty": req.frequency_penalty,
            "max_tokens": req.max_tokens,
            "presence_penalty": req.presence_penalty,
            "response_format": req.response_format,
            "stop": req.stop,
            "stream": req.stream,
            "stream_options": req.stream_options,
            "temperature": req.temperature,
            "top_p": req.top_p,
            "tools": req.tools,
            "tool_choice": req.tool_choice,
            "logprobs": req.logprobs,
            "top_logprobs": req.top_logprobs,
        }

        # Remove None fields
        data = {k: v for k, v in data.items() if v is not None}

        response = requests.post(api_url, json=data, headers=headers)
        try:
            response.raise_for_status()
            response_data = response.json()
        except Exception as e:
            print(f"API Response: {response.text}")
            raise e
        print(response_data)
        content = [
            LLMChatTextContent(text=response_data["choices"][0]["message"]["content"])
        ]
        return LLMChatResponse(
            model=req.model,
            usage=Usage(
                prompt_tokens=response_data["usage"]["prompt_tokens"],
                completion_tokens=response_data["usage"]["completion_tokens"],
                total_tokens=response_data["usage"]["total_tokens"],
            ),
            message=Message(
                content=content,
                role=response_data["choices"][0]["message"]["role"],
                tool_calls=response_data["choices"][0]["message"]["tool_calls"],
                finish_reason=response_data["choices"][0]["finish_reason"],
            ),
        )

    async def auto_detect_models(self) -> list[str]:
        api_url = f"{self.config.api_base}/models"
        async with aiohttp.ClientSession(trust_env=True) as session:
            async with session.get(
                api_url, headers={"Authorization": f"Bearer {self.config.api_key}"}
            ) as response:
                response.raise_for_status()
                response_data = await response.json()
                return [model["id"] for model in response_data["data"]]
